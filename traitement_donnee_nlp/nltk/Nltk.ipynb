{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n"
     ]
    }
   ],
   "source": [
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"And in early 2016, hackers at AnonSec claimed to have developed a method for gaining \\\n",
    "partial control over one of the Global Hawk drones used by NASA. Meanwhile, researchers at \\\n",
    "the Singapore University of Technology and Design have devised a technique for using drones \\\n",
    "to orchestrate MitM attacks which exploit wireless printing networks on a corporate scale, \\\n",
    "to eavesdrop on print jobs. The Flip Side: Counter-attack\\\\nAs with so many potential attack\\\n",
    "vectors, unmanned aerial vehicles may feature as a weapon in the armory of both defenders \\\n",
    "and attackers.For instance, the MalDrone backdoor malware kit has been developed as a \\\n",
    "universal hack, applicable to all makes and models of UAV.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization using punkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons de séparer les phrases du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And in early 2016, hackers at AnonSec claimed to have developed a method for gaining partial control over one of the Global Hawk drones used by NASA.',\n",
       " 'Meanwhile, researchers at the Singapore University of Technology and Design have devised a technique for using drones to orchestrate MitM attacks which exploit wireless printing networks on a corporate scale, to eavesdrop on print jobs.',\n",
       " 'The Flip Side: Counter-attack\\\\nAs with so many potential attackvectors, unmanned aerial vehicles may feature as a weapon in the armory of both defenders and attackers.For instance, the MalDrone backdoor malware kit has been developed as a universal hack, applicable to all makes and models of UAV.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ca ne marche pas ! NLTK a en effet d'un modèle punkt entrainé sur un large corpus du langage en question.\n",
    "-  apprend les mots de début de phrase\n",
    "- abbreviations, collocations\n",
    "\n",
    "#### Heureusement, récupérer les ressources sur un certain langage se fait très facilement avec nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pierre.leroy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And in early 2016, hackers at AnonSec claimed to have developed a method for gaining partial control over one of the Global Hawk drones used by NASA.',\n",
       " 'Meanwhile, researchers at the Singapore University of Technology and Design have devised a technique for using drones to orchestrate MitM attacks which exploit wireless printing networks on a corporate scale, to eavesdrop on print jobs.',\n",
       " 'The Flip Side: Counter-attack\\\\nAs with so many potential attackvectors, unmanned aerial vehicles may feature as a weapon in the armory of both defenders and attackers.For instance, the MalDrone backdoor malware kit has been developed as a universal hack, applicable to all makes and models of UAV.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- le saut de paragraphe est mal géré, ni l'oubli d'espace entre attacker et For"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et en français ? A télécharger un modèle entrainé sur le web, ou passer par un autre tokenizer\n",
    "- pour beaucoup de software de NLP, l'application en français est moins immédiate, et souvent un peu moins performante, que l'anglais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'in', 'early', '2016', ',', 'hackers', 'at', 'AnonSec', 'claimed', 'to', 'have', 'developed', 'a', 'method', 'for', 'gaining', 'partial', 'control', 'over', 'one', 'of', 'the', 'Global', 'Hawk', 'drones', 'used', 'by', 'NASA', '.', 'Meanwhile', ',', 'researchers', 'at', 'the', 'Singapore', 'University', 'of', 'Technology', 'and', 'Design', 'have', 'devised', 'a', 'technique', 'for', 'using', 'drones', 'to', 'orchestrate', 'MitM', 'attacks', 'which', 'exploit', 'wireless', 'printing', 'networks', 'on', 'a', 'corporate', 'scale', ',', 'to', 'eavesdrop', 'on', 'print', 'jobs', '.', 'The', 'Flip', 'Side', ':', 'Counter-attack\\\\nAs', 'with', 'so', 'many', 'potential', 'attackvectors', ',', 'unmanned', 'aerial', 'vehicles', 'may', 'feature', 'as', 'a', 'weapon', 'in', 'the', 'armory', 'of', 'both', 'defenders', 'and', 'attackers.For', 'instance', ',', 'the', 'MalDrone', 'backdoor', 'malware', 'kit', 'has', 'been', 'developed', 'as', 'a', 'universal', 'hack', ',', 'applicable', 'to', 'all', 'makes', 'and', 'models', 'of', 'UAV', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- le retour à la ligne pose ici également problème"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bordeaux', \"'s\", 'sky', 'blue', 'sky', 'is', '-according', 'to', 'me-', '1.5', 'darker', 'than', 'New', 'York', \"'s\", '.']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"Bordeaux's sky blue sky is -according to me- 1.5 darker than New York's .\"\n",
    "print(word_tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer, WordPunctTokenizer, WhitespaceTokenizer\n",
    "tree_tokenizer = TreebankWordTokenizer()\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "white_space_tokenizer = WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bordeaux', \"'s\", 'sky', 'blue', 'sky', 'is', '-according', 'to', 'me-', '1.5', 'darker', 'than', 'New', 'York', \"'s\", '.']\n"
     ]
    }
   ],
   "source": [
    "print(tree_tokenizer.tokenize(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word punct separe la punctuation, ce qui permet de correctement séparer mes \"-\", mais sépare les ' en des tokens séparés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enlever les stops words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pierre.leroy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "french_stopwords = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eut', 'son', 'était', 'mais', 'eux', 'eussent', 'on', 'l', 'serais', 'aie', 'se', 's', 'serez', 'fussions', 'été', 'eût', 'eûmes', 'il', 'leur', 'sa', 'pour', 'eusses', 'serons', 'fussiez', 'auras', 'des', 'notre', 'es', 'ou', 'fut', 'eu', 'fusse', 'fûmes', 'étions', 'nos', 'et', 'eurent', 'avez', 'aurais', 'ait', 'de', 'étantes', 'seriez', 'seront', 'ai', 'avons', 'ayons', 'la', 'avec', 'seras', 'pas', 'vos', 'y', 'ayant', 'aurai', 'aient', 'te', 'soient', 'ses', 'qui', 'je', 'étaient', 'auront', 'ont', 'elle', 'fûtes', 'eussiez', 'étées', 't', 'ne', 'd', 'sur', 'sont', 'en', 'tu', 'soit', 'avait', 'les', 'qu', 'ayante', 'nous', 'vous', 'sommes', 'aviez', 'eussions', 'j', 'votre', 'étais', 'serions', 'aurez', 'étiez', 'avions', 'fusses', 'fût', 'du', 'étants', 'sois', 'même', 'n', 'tes', 'êtes', 'étée', 'eue', 'dans', 'ma', 'ta', 'sera', 'soyons', 'avais', 'aurait', 'ton', 'aies', 'étant', 'c', 'eus', 'avaient', 'furent', 'fus', 'que', 'aurons', 'toi', 'auraient', 'as', 'à', 'moi', 'aux', 'ce', 'mon', 'serai', 'soyez', 'eûtes', 'par', 'le', 'mes', 'ils', 'est', 'fussent', 'seraient', 'une', 'lui', 'étante', 'm', 'ces', 'ayez', 'un', 'étés', 'auriez', 'suis', 'ayantes', 'ayants', 'eues', 'au', 'me', 'serait', 'eusse', 'aurions', 'aura'}\n"
     ]
    }
   ],
   "source": [
    "print(french_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enlevons les stopswords au moyen des compréhensions de listes en python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'early', '2016', ',', 'hackers', 'AnonSec', 'claimed', 'developed', 'method', 'gaining', 'partial', 'control', 'Global', 'Hawk', 'drones', 'NASA', '.', 'Meanwhile', ',', 'researchers', 'Singapore', 'University', 'Technology', 'Design', 'devised', 'technique', 'drones', 'orchestrate', 'MitM', 'attacks', 'exploit', 'wireless', 'printing', 'networks', 'corporate', 'scale', ',', 'eavesdrop', 'print', 'jobs', '.', 'The', 'Flip', 'Side', ':', 'Counter-attack\\\\nAs', 'potential', 'attackvectors', ',', 'unmanned', 'aerial', 'vehicles', 'feature', 'weapon', 'armory', 'defenders', 'attackers.For', 'instance', ',', 'MalDrone', 'backdoor', 'malware', 'kit', 'developed', 'universal', 'hack', ',', 'applicable', 'makes', 'models', 'UAV', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_text = [word for word in word_tokenize(text) if word not in stopwords.words()]\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- les imprécisions de chaque étape du pipeline impactent les étapes suivantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "snowball_stemmer = SnowballStemmer('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lie\n",
      "lie\n",
      "lie\n"
     ]
    }
   ],
   "source": [
    "print(porter_stemmer.stem(\"lying\"))\n",
    "print(porter_stemmer.stem(\"lies\"))\n",
    "print(porter_stemmer.stem(\"lied\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lying\n",
      "lie\n",
      "lied\n"
     ]
    }
   ],
   "source": [
    "print(lancaster_stemmer.stem(\"lying\"))\n",
    "print(lancaster_stemmer.stem(\"lies\"))\n",
    "print(lancaster_stemmer.stem(\"lied\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "test\n",
      "testent\n",
      "test\n",
      "tant\n"
     ]
    }
   ],
   "source": [
    "print(snowball_stemmer.stem(\"testé\"))\n",
    "print(snowball_stemmer.stem(\"teste\"))\n",
    "print(snowball_stemmer.stem(\"testent\"))\n",
    "print(snowball_stemmer.stem(\"testant\"))\n",
    "print(snowball_stemmer.stem(\"tant\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- meilleur qu'une implémentation naive\n",
    "- ce n'est pas parfait non plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pierre.leroy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"feet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "running\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"running\"))\n",
    "print(lemmatizer.lemmatize(\"running\",pos='n'))\n",
    "print(lemmatizer.lemmatize(\"running\",pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it may gain from tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running\n",
      "Running\n",
      "Running\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"Running\"))\n",
    "print(lemmatizer.lemmatize(\"Running\",pos='n'))\n",
    "print(lemmatizer.lemmatize(\"Running\",pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lemma = [lemmatizer.lemmatize(word) for word in filtered_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the differences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hackers hacker\n",
      "drones drone\n",
      "researchers researcher\n",
      "drones drone\n",
      "attacks attack\n",
      "networks network\n",
      "jobs job\n",
      "vehicles vehicle\n",
      "defenders defender\n",
      "makes make\n",
      "models model\n"
     ]
    }
   ],
   "source": [
    "for a,b in zip(filtered_text, text_lemma):\n",
    "    if a !=b: print(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('And', 'CC'), ('in', 'IN'), ('early', 'JJ'), ('2016', 'CD'), (',', ','), ('hackers', 'NNS'), ('at', 'IN'), ('AnonSec', 'NNP'), ('claimed', 'VBD'), ('to', 'TO'), ('have', 'VB'), ('developed', 'VBN'), ('a', 'DT'), ('method', 'NN'), ('for', 'IN'), ('gaining', 'VBG'), ('partial', 'JJ'), ('control', 'NN'), ('over', 'IN'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('Global', 'NNP'), ('Hawk', 'NNP'), ('drones', 'NNS'), ('used', 'VBN'), ('by', 'IN'), ('NASA', 'NNP'), ('.', '.'), ('Meanwhile', 'RB'), (',', ','), ('researchers', 'NNS'), ('at', 'IN'), ('the', 'DT'), ('Singapore', 'NNP'), ('University', 'NNP'), ('of', 'IN'), ('Technology', 'NNP'), ('and', 'CC'), ('Design', 'NNP'), ('have', 'VBP'), ('devised', 'VBN'), ('a', 'DT'), ('technique', 'NN'), ('for', 'IN'), ('using', 'VBG'), ('drones', 'NNS'), ('to', 'TO'), ('orchestrate', 'VB'), ('MitM', 'NNP'), ('attacks', 'NNS'), ('which', 'WDT'), ('exploit', 'VBP'), ('wireless', 'NN'), ('printing', 'NN'), ('networks', 'NNS'), ('on', 'IN'), ('a', 'DT'), ('corporate', 'JJ'), ('scale', 'NN'), (',', ','), ('to', 'TO'), ('eavesdrop', 'VB'), ('on', 'IN'), ('print', 'NN'), ('jobs', 'NNS'), ('.', '.'), ('The', 'DT'), ('Flip', 'NNP'), ('Side', 'NNP'), (':', ':'), ('Counter-attack\\\\nAs', 'JJ'), ('with', 'IN'), ('so', 'RB'), ('many', 'JJ'), ('potential', 'JJ'), ('attackvectors', 'NNS'), (',', ','), ('unmanned', 'JJ'), ('aerial', 'JJ'), ('vehicles', 'NNS'), ('may', 'MD'), ('feature', 'VB'), ('as', 'IN'), ('a', 'DT'), ('weapon', 'NN'), ('in', 'IN'), ('the', 'DT'), ('armory', 'NN'), ('of', 'IN'), ('both', 'DT'), ('defenders', 'NNS'), ('and', 'CC'), ('attackers.For', 'JJ'), ('instance', 'NN'), (',', ','), ('the', 'DT'), ('MalDrone', 'NNP'), ('backdoor', 'NN'), ('malware', 'NN'), ('kit', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('developed', 'VBN'), ('as', 'IN'), ('a', 'DT'), ('universal', 'JJ'), ('hack', 'NN'), (',', ','), ('applicable', 'JJ'), ('to', 'TO'), ('all', 'DT'), ('makes', 'VBZ'), ('and', 'CC'), ('models', 'NNS'), ('of', 'IN'), ('UAV', 'NNP'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\pierre.leroy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "print(pos_tag(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- don't remove the stopwords and lemma for precise tagging !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('And', 'CC'), ('early', 'JJ'), ('2016', 'CD'), (',', ','), ('hacker', 'NN'), ('AnonSec', 'NNP'), ('claimed', 'VBD'), ('developed', 'VBN'), ('method', 'NN'), ('gaining', 'VBG'), ('partial', 'JJ'), ('control', 'NN'), ('Global', 'NNP'), ('Hawk', 'NNP'), ('drone', 'NN'), ('NASA', 'NNP'), ('.', '.'), ('Meanwhile', 'RB'), (',', ','), ('researcher', 'JJR'), ('Singapore', 'NNP'), ('University', 'NNP'), ('Technology', 'NNP'), ('Design', 'NNP'), ('devised', 'VBD'), ('technique', 'NN'), ('drone', 'NN'), ('orchestrate', 'NN'), ('MitM', 'NNP'), ('attack', 'NN'), ('exploit', 'NN'), ('wireless', 'NN'), ('printing', 'NN'), ('network', 'NN'), ('corporate', 'JJ'), ('scale', 'NN'), (',', ','), ('eavesdrop', 'FW'), ('print', 'NN'), ('job', 'NN'), ('.', '.'), ('The', 'DT'), ('Flip', 'NNP'), ('Side', 'NNP'), (':', ':'), ('Counter-attack\\\\nAs', 'JJ'), ('potential', 'JJ'), ('attackvectors', 'NNS'), (',', ','), ('unmanned', 'JJ'), ('aerial', 'JJ'), ('vehicle', 'NN'), ('feature', 'NN'), ('weapon', 'IN'), ('armory', 'JJ'), ('defender', 'NN'), ('attackers.For', 'NN'), ('instance', 'NN'), (',', ','), ('MalDrone', 'NNP'), ('backdoor', 'NN'), ('malware', 'NN'), ('kit', 'NN'), ('developed', 'VBD'), ('universal', 'JJ'), ('hack', 'NN'), (',', ','), ('applicable', 'JJ'), ('make', 'VBP'), ('model', 'NN'), ('UAV', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(text_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\pierre.leroy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\pierre.leroy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  And/CC\n",
      "  in/IN\n",
      "  early/JJ\n",
      "  2016/CD\n",
      "  ,/,\n",
      "  hackers/NNS\n",
      "  at/IN\n",
      "  (ORGANIZATION AnonSec/NNP)\n",
      "  claimed/VBD\n",
      "  to/TO\n",
      "  have/VB\n",
      "  developed/VBN\n",
      "  a/DT\n",
      "  method/NN\n",
      "  for/IN\n",
      "  gaining/VBG\n",
      "  partial/JJ\n",
      "  control/NN\n",
      "  over/IN\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Global/NNP Hawk/NNP)\n",
      "  drones/NNS\n",
      "  used/VBN\n",
      "  by/IN\n",
      "  (ORGANIZATION NASA/NNP)\n",
      "  ./.\n",
      "  Meanwhile/RB\n",
      "  ,/,\n",
      "  researchers/NNS\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Singapore/NNP University/NNP)\n",
      "  of/IN\n",
      "  (ORGANIZATION Technology/NNP)\n",
      "  and/CC\n",
      "  Design/NNP\n",
      "  have/VBP\n",
      "  devised/VBN\n",
      "  a/DT\n",
      "  technique/NN\n",
      "  for/IN\n",
      "  using/VBG\n",
      "  drones/NNS\n",
      "  to/TO\n",
      "  orchestrate/VB\n",
      "  (ORGANIZATION MitM/NNP)\n",
      "  attacks/NNS\n",
      "  which/WDT\n",
      "  exploit/VBP\n",
      "  wireless/NN\n",
      "  printing/NN\n",
      "  networks/NNS\n",
      "  on/IN\n",
      "  a/DT\n",
      "  corporate/JJ\n",
      "  scale/NN\n",
      "  ,/,\n",
      "  to/TO\n",
      "  eavesdrop/VB\n",
      "  on/IN\n",
      "  print/NN\n",
      "  jobs/NNS\n",
      "  ./.\n",
      "  The/DT\n",
      "  (ORGANIZATION Flip/NNP)\n",
      "  Side/NNP\n",
      "  :/:\n",
      "  Counter-attack\\nAs/JJ\n",
      "  with/IN\n",
      "  so/RB\n",
      "  many/JJ\n",
      "  potential/JJ\n",
      "  attackvectors/NNS\n",
      "  ,/,\n",
      "  unmanned/JJ\n",
      "  aerial/JJ\n",
      "  vehicles/NNS\n",
      "  may/MD\n",
      "  feature/VB\n",
      "  as/IN\n",
      "  a/DT\n",
      "  weapon/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  armory/NN\n",
      "  of/IN\n",
      "  both/DT\n",
      "  defenders/NNS\n",
      "  and/CC\n",
      "  attackers.For/JJ\n",
      "  instance/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  (ORGANIZATION MalDrone/NNP)\n",
      "  backdoor/NN\n",
      "  malware/NN\n",
      "  kit/NN\n",
      "  has/VBZ\n",
      "  been/VBN\n",
      "  developed/VBN\n",
      "  as/IN\n",
      "  a/DT\n",
      "  universal/JJ\n",
      "  hack/NN\n",
      "  ,/,\n",
      "  applicable/JJ\n",
      "  to/TO\n",
      "  all/DT\n",
      "  makes/VBZ\n",
      "  and/CC\n",
      "  models/NNS\n",
      "  of/IN\n",
      "  (ORGANIZATION UAV/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(ne_chunk(pos_tag(word_tokenize(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Vous obtiendrez une erreur sans le print(), due a jupyter notebook qui traduit le format présent comme un script et demande d'installer de quoi faire tourner ce script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction de dépendences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- il n'y en a pas dans nltk\n",
    "- propose un wrapper vers celui de Stanford Core NLP :\n",
    "            - from nltk.parse.stanford import StanfordDependencyParser\n",
    "- necessite d'avoir Stanford Core Nlp d'installé et de fournir les au StanfordDependencyParser les .jar correspondants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK ET Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('computer.n.01'), Synset('calculator.n.01')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "wordnet.synsets(\"computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('good.n.01'),\n",
       " Synset('good.n.02'),\n",
       " Synset('good.n.03'),\n",
       " Synset('commodity.n.01'),\n",
       " Synset('good.a.01'),\n",
       " Synset('full.s.06'),\n",
       " Synset('good.a.03'),\n",
       " Synset('estimable.s.02'),\n",
       " Synset('beneficial.s.01'),\n",
       " Synset('good.s.06'),\n",
       " Synset('good.s.07'),\n",
       " Synset('adept.s.01'),\n",
       " Synset('good.s.09'),\n",
       " Synset('dear.s.02'),\n",
       " Synset('dependable.s.04'),\n",
       " Synset('good.s.12'),\n",
       " Synset('good.s.13'),\n",
       " Synset('effective.s.04'),\n",
       " Synset('good.s.15'),\n",
       " Synset('good.s.16'),\n",
       " Synset('good.s.17'),\n",
       " Synset('good.s.18'),\n",
       " Synset('good.s.19'),\n",
       " Synset('good.s.20'),\n",
       " Synset('good.s.21'),\n",
       " Synset('well.r.01'),\n",
       " Synset('thoroughly.r.02')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela nous donne beaucoup de choix, on peut réduire ce choix si on a deja taggué notre phrase !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('good.n.01'),\n",
       " Synset('good.n.02'),\n",
       " Synset('good.n.03'),\n",
       " Synset('commodity.n.01')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"good\",\"n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- chaque synset correspond à un \"sens\", qui est définit par l'ensemble des mots ayant ce sens\n",
    "- la lettre correspond au tag du sy,set (n:nom, ...)\n",
    "\n",
    "#### Et qu'est ce que l'on peut faire de ces synsets  ?\n",
    "- on peut obtenir une description de la definiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'benefit'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('good.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moral excellence or admirableness'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('good.n.02').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that which is pleasing or valuable or useful'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('good.n.03').definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- les synonymes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'goodness']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('good.n.03').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['commodity', 'trade_good', 'good']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('commodity.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- les synsets sont indépendants du langage, ie on peut acceder au synset quel que soit le langage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\pierre.leroy\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Synset('possession.n.02'),\n",
       " Synset('possession.n.07'),\n",
       " Synset('commodity.n.01'),\n",
       " Synset('well.n.04'),\n",
       " Synset('good.n.03'),\n",
       " Synset('possession.n.05'),\n",
       " Synset('oklahoma.n.01'),\n",
       " Synset('property.n.01')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw')\n",
    "wordnet.synsets(\"bien\",\"n\",lang='fra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- et donc traduire si on sait le synset correspondant ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = [\"Mon\", \"chien\", \"est\", \"beau\"]\n",
    "eng_sen = [wordnet.synsets(word,lang=\"fra\")[0].lemma_names()[0] for word in sen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['watch', 'dog', 'east', 'beautiful']\n"
     ]
    }
   ],
   "source": [
    "print(eng_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- on pourrait améliorer grâce au tag\n",
    "- choisir le bon synset dépend du contexte du mot : on atteint les limites de cette approche pour la traduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cela nous donne une notion de distance basée sur le sens des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset1 = wordnet.synset('dog.n.01')\n",
    "synset2 = wordnet.synset('cat.n.01')\n",
    "synset1.path_similarity(synset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset1 = wordnet.synset('dog.n.01')\n",
    "synset2 = wordnet.synset('llama.n.01')\n",
    "synset1.path_similarity(synset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09090909090909091"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset1 = wordnet.synset('cat.n.01')\n",
    "synset2 = wordnet.synset('tiger.n.01')\n",
    "synset1.path_similarity(synset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset1 = wordnet.synset('hit.v.01')\n",
    "synset2 = wordnet.synset('slap.v.01')\n",
    "synset1.path_similarity(synset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- il existe d'autres distances (non parfaites)\n",
    "- le DL nous permettra de faire cela de manière plus efficace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordnet est plutôt à considéré comme un dictionnaire disponible multi-langage, à utiliser comme tel !\n",
    "- de nombreuses fonction intéressantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('computer_science.n.01')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('code.n.03').topic_domains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('slang.n.02')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('freaky.a.01').usage_domains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
